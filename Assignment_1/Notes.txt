    Notes for Time:

Times (measured on Gantenbein):
-> Compiled with -O0:
    between 2.378s and 2.459s    
-> Compiled with -O1:
    between 0.265s and 0.281s
-> Compiled with -O2:
    between 939ns and 3336ns
-> Compiled with -O3:
    between 946ns and 3000ns
-> Compiled with -Os:
    between 0.268s and 0.270s
-> Compiled with -Og:
    between 0.535s and 0.552s

    Notes to time assembly code:
the higher the optimization level, the smaller the assembly code
which makes sense but is also an indication that it is harder to 
find a bug in the optimized assembly code


#### #### #### #### #### #### #### #### #### #### #### #### #### #### #### 


    Notes for Inlining:
To get some time differences, every call of one of these programs starts 10.000 calculations.

[Benchmarked the 3 variants with hyperfine on Gantenbein]
mainfile
    mean: 481,1 ms +/-13,8 ms
    min ... max: 458,3ms ... 505,1ms

separatefile
    mean: 591,3 ms +/-8,2 ms
    min ... max: 580,9ms ... 604ms

inlined
    mean: 466,1 ms +/-12,4 ms
    min ... max: 447,3ms ... 485,3ms

The time differences make sence: inlined code is faster than calling a method in mainfile.
Separatefile is the slowest method since it calls a method all the time (symbol mul_cpx is still in nm output)


using nm to find examine the executables mul_cpx_mainfile and mul_cpx_separatefile:
nm mul_cpx_mainfile:
    NOTE: we have to use static inline on mul_cpx_mainfile to get rid of the mul_cpx method name!
nm mul_cpx_separatefile:
    I find the method name "mul_cpx" as well as "main" -> which is an indication that separatefile
    calls a method mul_cps!!


#### #### #### #### #### #### #### #### #### #### #### #### #### #### #### 


    Notes for Locality:

Benchmarking both with timespec_get time on Gantenbein (-O2):
row sum: ~1.7ms
col sum: ~2.0ms with larger deviations ranging from 1.6ms up to 3.3
    which makes sense since we store the data in row order

    --> Reimplement slower (column) summation...
I improved the efficiency by iterating in the same way as in the rows_sum so I can
use the data-locality. I have to jump to different sums[j] all the time but they are
also stored sequentially so this is not really a problem.

    achieved Timing - Improvement:
col2 sum: ~ 0.85ms
so we are even faster than in the row sum. This surprised me a little bit and after 
comparing the code, the only difference is that the row_sum sums first into a local variable
before writing this sum into the sum array. It seems that writing directly to the array 
is more efficient.


    Notes on -O0 compilation:
Running the un-optimized program results in nearly the same execution time for the given
row_sums and col_sums implementations. However, my improved version is notably faster than
the row_sums implementation which surprised me. 
Since the only real difference is the omission of the local sum calculation and instead writing directly to the given sum-array, this has to be the problem here.



#### #### #### #### #### #### #### #### #### #### #### #### #### #### #### 


        Notes for Indirect Adressing:

    Times with -O2 flag:
'random' indirect adressing:    1s 373ms
sequential indirect adressing:  0s 671ms
direct adressing:               0s 560ms

    Times with -O0 flag:
'random' indirect adressing:    3s 243ms
sequential indirect adressing:  2s 936ms
direct adressing:               2s 587ms

Indirect adressing is slower than direct adressing which makes sense.
We can also see that the compiler does a lot of optimization in the sequential indirect
adressing case which gets nearly as fast as the direct adressing case.

Difference -O2 vs -O0: notably, compiler can do a lot especially in direct adressing & seq. indirect cases!



#### #### #### #### #### #### #### #### #### #### #### #### #### #### #### 

        Notes on HDD vs SSD:


