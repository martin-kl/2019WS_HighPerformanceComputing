    Notes for Time:

Times (measured on Gantenbein) ON OLD_VERSION (printing time after every sum calc):
-> Compiled with -O0:
    between 2.378s and 2.459s    
-> Compiled with -O1:
    between 0.265s and 0.281s
-> Compiled with -O2:
    between 939ns and 3336ns
-> Compiled with -O3:
    between 946ns and 3000ns
-> Compiled with -Os:
    between 0.268s and 0.270s
-> Compiled with -Og:
    between 0.535s and 0.552s

Time on NEW VERSION (updated assignment description containing looping and avg calculation):
-> Compiled with -O0:
   2394.45ms    on avg over 100 iterations 
-> Compiled with -O1:
   287.35ms     on avg over 100 iterations 
-> Compiled with -O2:
    0.001390ms  on avg over 100 Iterations
-> Compiled with -O3:
    0.001446ms  on avg over 100 Iterations
-> Compiled with -Os:
    0.001365ms  on avg over 100 Iterations
-> Compiled with -Og:
    555.21ms  on avg over 100 Iterations

    Notes to time assembly code:
the higher the optimization level, the smaller the assembly code
which makes sense but is also an indication that it is harder to 
find a bug in the optimized assembly code


#### #### #### #### #### #### #### #### #### #### #### #### #### #### #### 


    Notes for Inlining:
To get some time differences, every call of one of these programs starts 10.000 calculations.

[Benchmarked the 3 variants with hyperfine on Gantenbein]
mainfile
    mean: 481,1 ms +/-13,8 ms
    min ... max: 458,3ms ... 505,1ms

separatefile
    mean: 591,3 ms +/-8,2 ms
    min ... max: 580,9ms ... 604ms

inlined
    mean: 466,1 ms +/-12,4 ms
    min ... max: 447,3ms ... 485,3ms

The time differences make sence: inlined code is faster than calling a method in mainfile.
Separatefile is the slowest method since it calls a method all the time (symbol mul_cpx is still in nm output)


using nm to find examine the executables mul_cpx_mainfile and mul_cpx_separatefile:
nm mul_cpx_mainfile:
    NOTE: we have to use static inline on mul_cpx_mainfile to get rid of the mul_cpx method name!
nm mul_cpx_separatefile:
    I find the method name "mul_cpx" as well as "main" -> which is an indication that separatefile
    calls a method mul_cps!!


#### #### #### #### #### #### #### #### #### #### #### #### #### #### #### 


    Notes for Locality:

Benchmarking both with timespec_get time on Gantenbein (-O2) and 100 iterations each:
row sum: 1.11ms
col sum: 1.56ms which makes sense since we store the data in row order

    --> Reimplement slower (column) summation...
I improved the efficiency by iterating in the same way as in the rows_sum so I can
use the data-locality. I have to jump to different sums[j] all the time but they are
also stored sequentially => not really a problem.

    achieved Timing - Improvement:
col2 sum: 0.57ms
so we are notably faster than in the row sum. This surprised me a little bit and after 
comparing the code, the only difference is that the row_sum sums first into a local variable
before writing this sum into the sum array. It seems that writing directly to the array 
is way  more efficient.


    Notes on -O0 compilation:
Running the un-optimized program results in nearly the same execution time for the given
row_sums and col_sums implementations. However, my improved version is notably faster than
the row_sums implementation which surprised me. 
Since the only real difference is the omission of the local sum calculation and instead writing directly to the given sum-array, this has to be the problem here.



#### #### #### #### #### #### #### #### #### #### #### #### #### #### #### 


        Notes for Indirect Adressing:

    Times with -O2 flag:
'random' indirect adressing:    1s 373ms
sequential indirect adressing:  0s 671ms
direct adressing:               0s 560ms

    Times with -O0 flag:
'random' indirect adressing:    3s 243ms
sequential indirect adressing:  2s 936ms
direct adressing:               2s 587ms

Indirect adressing is slower than direct adressing which makes sense.
We can also see that the compiler does a lot of optimization in the sequential indirect
adressing case which gets nearly as fast as the direct adressing case.

Difference -O2 vs -O0: notably, compiler can do a lot especially in direct adressing & seq. indirect cases!



#### #### #### #### #### #### #### #### #### #### #### #### #### #### #### 

        Notes on HDD vs SSD:
    Times with -O2 flag:
HDD W:  66ms
HDD R:  61ms
SSD W:  55ms
SSD R:  61ms

Writing to SSD is faster (as expected) but reading from SSD is as fast as reading from an HDD RAID which is quite surprising to me !!!
[I tried this multiple times while removing the files between the runs and got this result consistently]

Possibly since these are fast(er) Server-Disks + the RAID 5?!

copy the folder /usr/include to your home director,
.) measure the time of copying 10 times this copy to another folder called include_copy,
    used "time" function and copy script that copies folder & deletes it 10 times
    real:   7.444s
    user:   0.799
    sys:    6.521s

copy the folder /usr/include to your directory in /run/mount/scratch,
.) measure the time of copying 10 times this copy to another folder called include_copy.
    real::  6.217s
    user:   0.743s
    sys:    5.360s

We see again that copying is indeed faster on the SSD than on the HDD but this HDD does not
seem to be a slow one since the difference is not that huge.
Normally SSDs are way faster on random access but here we had only sequential IO.



#### #### #### #### #### #### #### #### #### #### #### #### #### #### #### 

        Notes on Valgrind:

Compile the program with optimization level 0 and with the flag -g, then run the program with valgrind memcheck.

Repeat compilation & running with valgrind after making one of the following modifications:
.) Comment out the initialization of as.
valgrind detects the use of an uninitialized value on the first access to the array.

.) Comment out the freeing of as.
valgrind detects a memory leak with "definitely lost: 40 bytes in 1 blocks" and even tells us where this lost bytes are allocated.

.) Amend the code with an additional free(as) at the end.
valgrind detects an "Invalid free()..." operation and gives us the line again.



#### #### #### #### #### #### #### #### #### #### #### #### #### #### #### 

        Notes on GDB:

This starts the debugger gdb. Follow the next steps:

.) Press ‘r’ and enter to start running the program
.) Once the execution stops (because of invalid memory access), type “focus” and press enter.
.) Type “p as” and press enter to see the print the value of as at the time the program aborted execution.
    -> ATTENTION: I did not get any output besides "The history is empty."
